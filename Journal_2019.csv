Tittle,PublishedDate,Authors,Fulltext.Url,Abstract,Keywords,Affiliations,CorrospondingAuthors,CorrospondingAuthors.Emails
Kohdista: an efficient method to index and query possible Rmap alignments,12-Dec-19,"Martin D. Muggli, Simon J. Puglisi and Christina Boucher",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0160-9,"BackgroundGenome-wide optical maps are ordered high-resolution restriction maps that give the position of occurrence of restriction cut sites corresponding to one or more restriction enzymes. These genome-wide optical maps are assembled using an overlap-layout-consensus approach using raw optical map data, which are referred to as Rmaps. Due to the high error-rate of Rmap data, finding the overlap between Rmaps remains challenging.ResultsWe present Kohdista, which is an index-based algorithm for finding pairwise alignments between single molecule maps (Rmaps). The novelty of our approach is the formulation of the alignment problem as automaton path matching, and the application of modern index-based data structures. In particular, we combine the use of the Generalized Compressed Suffix Array (GCSA) index with the wavelet tree in order to build Kohdista. We validate Kohdista on simulated E. coli data, showing the approach successfully finds alignments between Rmaps simulated from overlapping genomic regions.Conclusionwe demonstrate Kohdista is the only method that is capable of finding a significant number of high quality pairwise Rmap alignments for large eukaryote organisms in reasonable time.","Optical mapping, Index based data structures, FM-index, Graph algorithms","Department of Computer Science, Colorado State University, Fort Collins, CO, USAMartin D. MuggliDepartment of Computer Science, University of Helsinki, Helsinki, FinlandSimon J. PuglisiComputer & Information Science & Engineering, University of Florida, Gainesville, FL, USAChristina Boucher",Christina Boucher,NA
NANUQ: a method for inferring species networks from gene trees under the coalescent model,06-Dec-19,"Elizabeth S. Allman, Hector Baños and John A. Rhodes",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0159-2,"Species networks generalize the notion of species trees to allow for hybridization or other lateral gene transfer. Under the network multispecies coalescent model, individual gene trees arising from a network can have any topology, but arise with frequencies dependent on the network structure and numerical parameters. We propose a new algorithm for statistical inference of a level-1 species network under this model, from data consisting of gene tree topologies, and provide the theoretical justification for it. The algorithm is based on an analysis of quartets displayed on gene trees, combining several statistical hypothesis tests with combinatorial ideas such as a quartet-based intertaxon distance appropriate to networks, the NeighborNet algorithm for circular split systems, and the Circular Network algorithm for constructing a splits graph.","Hybridization, Network multispecies coalescent, Species network inference, Gene tree, Quartets, Level-1 network, NANUQ, Primary 92B10, 92D15","Department of of Mathematics and Statistics, University of Alaska Fairbanks, 1792 Ambler Lane, Fairbanks, AK, 99775, USAElizabeth S. Allman, Hector Baños & John A. Rhodes",John A. Rhodes,NA
TMRS: an algorithm for computing the time to the most recent substitution event from a multiple alignment column,18-Nov-19,"Hisanori Kiryu, Yuto Ichikawa and Yasuhiro Kojima",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0158-3,"Background As the number of sequenced genomes grows, researchers have access to an increasingly rich source for discovering detailed evolutionary information. However, the computational technologies for inferring biologically important evolutionary events are not sufficiently developed.Results We present algorithms to estimate the evolutionary time (\(t_{\text {MRS}}\)) to the most recent substitution event from a multiple alignment column by using a probabilistic model of sequence evolution. As the confidence in estimated \(t_{\text {MRS}}\) values varies depending on gap fractions and nucleotide patterns of alignment columns, we also compute the standard deviation \(\sigma\) of \(t_{\text {MRS}}\) by using a dynamic programming algorithm. We identified a number of human genomic sites at which the last substitutions occurred between two speciation events in the human lineage with confidence. A large fraction of such sites have substitutions that occurred between the concestor nodes of Hominoidea and Euarchontoglires. We investigated the correlation between tissue-specific transcribed enhancers and the distribution of the sites with specific substitution time intervals, and found that brain-specific transcribed enhancers are threefold enriched in the density of substitutions in the human lineage relative to expectations.Conclusions We have presented algorithms to estimate the evolutionary time (\(t_{\text {MRS}}\)) to the most recent substitution event from a multiple alignment column by using a probabilistic model of sequence evolution. Our algorithms will be useful for Evo-Devo studies, as they facilitate screening potential genomic sites that have played an important role in the acquisition of unique biological features by target species.","Phylogenetic trees, Comparative genomics, Probabilistic models","Department of Computational Biology and Medical Sciences, GSFS, University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba, JapanHisanori Kiryu & Yasuhiro KojimaWorks Applications Co., Ltd., 1-12-32, Akasaka, Minato-ku, Tokyo, JapanYuto Ichikawa",Hisanori Kiryu,NA
Adjacency-constrained hierarchical clustering of a band similarity matrix with application to genomics,15-Nov-19,"Christophe Ambroise, Alia Dehman, Pierre Neuvial, Guillem Rigaill and Nathalie Vialaneix",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4,"BackgroundGenomic data analyses such as Genome-Wide Association Studies (GWAS) or Hi-C studies are often faced with the problem of partitioning chromosomes into successive regions based on a similarity matrix of high-resolution, locus-level measurements. An intuitive way of doing this is to perform a modified Hierarchical Agglomerative Clustering (HAC), where only adjacent clusters (according to the ordering of positions within a chromosome) are allowed to be merged. But a major practical drawback of this method is its quadratic time and space complexity in the number of loci, which is typically of the order of \(10^4\) to \(10^5\) for each chromosome.ResultsBy assuming that the similarity between physically distant objects is negligible, we are able to propose an implementation of adjacency-constrained HAC with quasi-linear complexity. This is achieved by pre-calculating specific sums of similarities, and storing candidate fusions in a min-heap. Our illustrations on GWAS and Hi-C datasets demonstrate the relevance of this assumption, and show that this method highlights biologically meaningful signals. Thanks to its small time and memory footprint, the method can be run on a standard laptop in minutes or even seconds.Availability and implementationSoftware and sample data are available as an R package, adjclust, that can be downloaded from the Comprehensive R Archive Network (CRAN).","Hierarchical agglomerative clustering, Adjacency constraint, Segmentation, Ward’s linkage, Similarity, Min heap, Genome-Wide Association Studies and Hi-C","Laboratoire de Mathématiques et Modélisation d’Evry, UMR CNRS 8071, Université d’Evry Val d’Essonne, 23 boulevard de France, 91037, Evry, FranceChristophe Ambroise & Guillem RigaillHyphen-stat, 195 Route d’Espagne, 31036, Toulouse, FranceAlia DehmanInstitut de Mathématiques de Toulouse, UMR5219 CNRS, Université de Toulouse, UPS IMT, 31062, Toulouse Cedex 9, FrancePierre NeuvialInstitute of Plant Sciences Paris Saclay IPS2, CNRS, INRA, Gif sur Yvette, FranceGuillem RigaillMIAT, Université de Toulouse, INRA, Castanet-Tolosan, FranceNathalie Vialaneix",Pierre Neuvial,NA
Super short operations on both gene order and intergenic sizes,05-Nov-19,"Andre R. Oliveira, Géraldine Jean, Guillaume Fertin, Ulisses Dias and Zanoni Dias",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0156-5,"BackgroundThe evolutionary distance between two genomes can be estimated by computing a minimum length sequence of operations, called genome rearrangements, that transform one genome into another. Usually, a genome is modeled as an ordered sequence of genes, and most of the studies in the genome rearrangement literature consist in shaping biological scenarios into mathematical models. For instance, allowing different genome rearrangements operations at the same time, adding constraints to these rearrangements (e.g., each rearrangement can affect at most a given number of genes), considering that a rearrangement implies a cost depending on its length rather than a unit cost, etc. Most of the works, however, have overlooked some important features inside genomes, such as the presence of sequences of nucleotides between genes, called intergenic regions.Results and conclusionsIn this work, we investigate the problem of computing the distance between two genomes, taking into account both gene order and intergenic sizes. The genome rearrangement operations we consider here are constrained types of reversals and transpositions, called super short reversals (SSRs) and super short transpositions (SSTs), which affect up to two (consecutive) genes. We denote by super short operations (SSOs) any SSR or SST. We show 3-approximation algorithms when the orientation of the genes is not considered when we allow SSRs, SSTs, or SSOs, and 5-approximation algorithms when considering the orientation for either SSRs or SSOs. We also show that these algorithms improve their approximation factors when the input permutation has a higher number of inversions, where the approximation factor decreases from 3 to either 2 or 1.5, and from 5 to either 3 or 2.","Genome rearrangements, Intergenic regions, Super short operations, Approximation algorithms","Institute of Computing, University of Campinas, Campinas, BrazilAndre R. Oliveira & Zanoni DiasLS2N, UMR CNRS 6004, University of Nantes, Nantes, FranceGéraldine Jean & Guillaume FertinSchool of Technology, University of Campinas, Limeira, BrazilUlisses Dias",Andre R. Oliveira,NA
Bayesian localization of CNV candidates in WGS data within minutes,23-Sep-19,"John Wiedenhoeft, Alex Cagan, Rimma Kozhemyakina, Rimma Gulevich and Alexander Schliep",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0154-7,"BackgroundFull Bayesian inference for detecting copy number variants (CNV) from whole-genome sequencing (WGS) data is still largely infeasible due to computational demands. A recently introduced approach to perform Forward–Backward Gibbs sampling using dynamic Haar wavelet compression has alleviated issues of convergence and, to some extent, speed. Yet, the problem remains challenging in practice.ResultsIn this paper, we propose an improved algorithmic framework for this approach. We provide new space-efficient data structures to query sufficient statistics in logarithmic time, based on a linear-time, in-place transform of the data, which also improves on the compression ratio. We also propose a new approach to efficiently store and update marginal state counts obtained from the Gibbs sampler.ConclusionsUsing this approach, we discover several CNV candidates in two rat populations divergently selected for tame and aggressive behavior, consistent with earlier results concerning the domestication syndrome as well as experimental observations. Computationally, we observe a 29.5-fold decrease in memory, an average 5.8-fold speedup, as well as a 191-fold decrease in minor page faults. We also observe that metrics varied greatly in the old implementation, but not the new one. We conjecture that this is due to the better compression scheme. The fully Bayesian segmentation of the entire WGS data set required 3.5 min and 1.24 GB of memory, and can hence be performed on a commodity laptop.","HMM, Wavelet, CNV, Bayesian inference","Department of Computer Science and Engineering, University of Gothenburg | Chalmers, Rännvägen 6, 412 58, Gothenburg, SwedenJohn Wiedenhoeft & Alexander SchliepDepartment of Computer Science, Rutgers University, Piscataway, NJ, 08854, USAJohn Wiedenhoeft & Alexander SchliepMax Planck Institute for Evolutionary Anthropology, 04103, Leipzig, GermanyAlex CaganWellcome Trust Sanger Institute, Hinxton, CB10 1SA, UKAlex CaganInstitute of Cytology and Genetics of the Siberian Branch of the Russian Academy of Sciences, Novosibirsk, 630090, RussiaRimma Kozhemyakina & Rimma Gulevich",John Wiedenhoeft,NA
Implications of non-uniqueness in phylogenetic deconvolution of bulk DNA samples of tumors,03-Sep-19,"Yuanyuan Qi, Dikshant Pradhan and Mohammed El-Kebir",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0155-6,"BackgroundTumors exhibit extensive intra-tumor heterogeneity, the presence of groups of cellular populations with distinct sets of somatic mutations. This heterogeneity is the result of an evolutionary process, described by a phylogenetic tree. In addition to enabling clinicians to devise patient-specific treatment plans, phylogenetic trees of tumors enable researchers to decipher the mechanisms of tumorigenesis and metastasis. However, the problem of reconstructing a phylogenetic tree T given bulk sequencing data from a tumor is more complicated than the classic phylogeny inference problem. Rather than observing the leaves of T directly, we are given mutation frequencies that are the result of mixtures of the leaves of T. The majority of current tumor phylogeny inference methods employ the perfect phylogeny evolutionary model. The underlying Perfect Phylogeny Mixture (PPM) combinatorial problem typically has multiple solutions.ResultsWe prove that determining the exact number of solutions to the PPM problem is #P-complete and hard to approximate within a constant factor. Moreover, we show that sampling solutions uniformly at random is hard as well. On the positive side, we provide a polynomial-time computable upper bound on the number of solutions and introduce a simple rejection-sampling based scheme that works well for small instances. Using simulated and real data, we identify factors that contribute to and counteract non-uniqueness of solutions. In addition, we study the sampling performance of current methods, identifying significant biases.ConclusionsAwareness of non-uniqueness of solutions to the PPM problem is key to drawing accurate conclusions in downstream analyses based on tumor phylogenies. This work provides the theoretical foundations for non-uniqueness of solutions in tumor phylogeny inference from bulk DNA samples.","Phylogenetics, Intra-tumor heterogeneity, Inter-tumor heterogeneity, Somatic mutations, Single-nucleotide variant, Copy-number aberration, Structural variant, Metastasis, Evolution","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, 61801, USAYuanyuan Qi & Mohammed El-KebirDepartment of Bioengineering, University of Illinois at Urbana-Champaign, Urbana, IL, 61801, USADikshant Pradhan",Mohammed El-Kebir,NA
"A branching process for homology distribution-based inference of polyploidy, speciation and loss",01-Aug-19,"Yue Zhang, Chunfang Zheng and David Sankoff",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0153-8,"BackgroundThe statistical distribution of the similarity or difference between pairs of paralogous genes, created by whole genome doubling, or between pairs of orthologous genes in two related species is an important source of information about genomic evolution, especially in plants.MethodsWe derive the mixture of distributions of sequence similarity for duplicate gene pairs generated by repeated episodes of whole gene doubling. This involves integrating sequence divergence and gene pair loss through fractionation, using a branching process and a mutational model. We account not only for the timing of these events in terms of local modes, but also the amplitude and variance of the component distributions. This model is then extended to orthologous gene pairs.ResultsWe apply the model and inference procedures to the evolution of the Solanaceae, focusing on the genomes of economically important crops. We assess how consistent or variable fractionation rates are from species to species and over time.","Branching process, Whole genome doubling, Fractionation, Solanaceae","Department of Mathematics and Statistics, University of Ottawa, 150 Louis Pasteur Pvt, Ottawa, K1N 6N5, CanadaYue Zhang, Chunfang Zheng & David Sankoff",David Sankoff,NA
A multi-labeled tree dissimilarity measure for comparing “clonal trees” of tumor progression,27-Jul-19,"Nikolai Karpov, Salem Malikic, Md. Khaledur Rahman and S. Cenk Sahinalp",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0152-9,"We introduce a new dissimilarity measure between a pair of “clonal trees”, each representing the progression and mutational heterogeneity of a tumor sample, constructed by the use of single cell or bulk high throughput sequencing data. In a clonal tree, each vertex represents a specific tumor clone, and is labeled with one or more mutations in a way that each mutation is assigned to the oldest clone that harbors it. Given two clonal trees, our multi-labeled tree dissimilarity (MLTD) measure is defined as the minimum number of mutation/label deletions, (empty) leaf deletions, and vertex (clonal) expansions, applied in any order, to convert each of the two trees to the maximum common tree. We show that the MLTD measure can be computed efficiently in polynomial time and it captures the similarity between trees of different clonal granularity well.","Intra-tumor heterogeneity, Tumor evolution, Multi-labeled tree, Tree edit distance, Dynamic programming","Department of Computer Science, Indiana University, Bloomington, IN, USANikolai Karpov, Md. Khaledur Rahman & S. Cenk SahinalpSchool of Computing Science, Simon Fraser University, Burnaby, BC, CanadaSalem Malikic",S. Cenk Sahinalp,NA
A cubic algorithm for the generalized rank median of three genomes,26-Jul-19,"Leonid Chindelevitch, Sean La and Joao Meidanis",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0150-y,"BackgroundThe area of genome rearrangements has given rise to a number of interesting biological, mathematical and algorithmic problems. Among these, one of the most intractable ones has been that of finding the median of three genomes, a special case of the ancestral reconstruction problem. In this work we re-examine our recently proposed way of measuring genome rearrangement distance, namely, the rank distance between the matrix representations of the corresponding genomes, and show that the median of three genomes can be computed exactly in polynomial time \(O(n^\omega )\), where \(\omega \le 3\), with respect to this distance, when the median is allowed to be an arbitrary orthogonal matrix.ResultsWe define the five fundamental subspaces depending on three input genomes, and use their properties to show that a particular action on each of these subspaces produces a median. In the process we introduce the notion of M-stable subspaces. We also show that the median found by our algorithm is always orthogonal, symmetric, and conserves any adjacencies or telomeres present in at least 2 out of 3 input genomes.ConclusionsWe test our method on both simulated and real data. We find that the majority of the realistic inputs result in genomic outputs, and for those that do not, our two heuristics perform well in terms of reconstructing a genomic matrix attaining a score close to the lower bound, while running in a reasonable amount of time. We conclude that the rank distance is not only theoretically intriguing, but also practically useful for median-finding, and potentially ancestral genome reconstruction.","Comparative genomics, Ancestral genome reconstruction, Phylogenetics, Rank distance","School of Computing Science, Simon Fraser University, Burnaby, CanadaLeonid Chindelevitch & Sean LaInstitute of Computing, University of Campinas, Campinas, BrazilJoao Meidanis",Leonid Chindelevitch,NA
A general framework for genome rearrangement with biological constraints,19-Jul-19,"Pijus Simonaitis, Annie Chateau and Krister M. Swenson",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0149-4,"This paper generalizes previous studies on genome rearrangement under biological constraints, using double cut and join (DCJ). We propose a model for weighted DCJ, along with a family of optimization problems called \(\varphi\)-MCPS (Minimum Cost Parsimonious Scenario), that are based on labeled graphs. We show how to compute solutions to general instances of \(\varphi\)-MCPS, given an algorithm to compute \(\varphi\)-MCPS on a circular genome with exactly one occurrence of each gene. These general instances can have an arbitrary number of circular and linear chromosomes, and arbitrary gene content. The practicality of the framework is displayed by presenting polynomial-time algorithms that generalize the results of Bulteau, Fertin, and Tannier on the Sorting by wDCJs and indels in intergenes problem, and that generalize previous results on the Minimum Local Parsimonious Scenario problem.","Double cut and join (DCJ), Weighted genome rearrangement, Breakpoint graph, Graph edit distance, Edge switch","CNRS, LIRMM, Université Montpellier, 161 Rue Ada, 34392, Montpellier, FrancePijus Simonaitis, Annie Chateau & Krister M. SwensonInstitut de Biologie Computationnelle (IBC), Montpellier, FranceAnnie Chateau & Krister M. Swenson",Krister M. Swenson,NA
Statistically consistent divide-and-conquer pipelines for phylogeny estimation using NJMerge,19-Jul-19,Erin K. Molloy and Tandy Warnow,https://almob.biomedcentral.com/articles/10.1186/s13015-019-0151-x,"BackgroundDivide-and-conquer methods, which divide the species set into overlapping subsets, construct a tree on each subset, and then combine the subset trees using a supertree method, provide a key algorithmic framework for boosting the scalability of phylogeny estimation methods to large datasets. Yet the use of supertree methods, which typically attempt to solve NP-hard optimization problems, limits the scalability of such approaches.ResultsIn this paper, we introduce a divide-and-conquer approach that does not require supertree estimation: we divide the species set into pairwise disjoint subsets, construct a tree on each subset using a base method, and then combine the subset trees using a distance matrix. For this merger step, we present a new method, called NJMerge, which is a polynomial-time extension of Neighbor Joining (NJ); thus, NJMerge can be viewed either as a method for improving traditional NJ or as a method for scaling the base method to larger datasets. We prove that NJMerge can be used to create divide-and-conquer pipelines that are statistically consistent under some models of evolution. We also report the results of an extensive simulation study evaluating NJMerge on multi-locus datasets with up to 1000 species. We found that NJMerge sometimes improved the accuracy of traditional NJ and substantially reduced the running time of three popular species tree methods (ASTRAL-III, SVDquartets, and “concatenation” using RAxML) without sacrificing accuracy. Finally, although NJMerge can fail to return a tree, in our experiments, NJMerge failed on only 11 out of 2560 test cases.ConclusionsTheoretical and empirical results suggest that NJMerge is a valuable technique for large-scale phylogeny estimation, especially when computational resources are limited. NJMerge is freely available on Github (http://github.com/ekmolloy/njmerge).","Divide-and-conquer, Neighbor Joining, Species trees, Incomplete lineage sorting, Phylogenomics","Department of Computer Science, University of Illinois at Urbana-Champaign, 201 North Goodwin Avenue, Urbana, IL, 61801, USAErin K. Molloy & Tandy Warnow",Erin K. Molloy,NA
Prefix-free parsing for building big BWTs,24-May-19,"Christina Boucher, Travis Gagie, Alan Kuhnle, Ben Langmead, Giovanni Manzini and Taher Mun",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0148-5,"High-throughput sequencing technologies have led to explosive growth of genomic databases; one of which will soon reach hundreds of terabytes. For many applications we want to build and store indexes of these databases but constructing such indexes is a challenge. Fortunately, many of these genomic databases are highly-repetitive—a characteristic that can be exploited to ease the computation of the Burrows-Wheeler Transform (BWT), which underlies many popular indexes. In this paper, we introduce a preprocessing algorithm, referred to as prefix-free parsing, that takes a text T as input, and in one-pass generates a dictionary D and a parse P of T with the property that the BWT of T can be constructed from D and P using workspace proportional to their total size and O(|T|)-time. Our experiments show that D and P are significantly smaller than T in practice, and thus, can fit in a reasonable internal memory even when T is very large. In particular, we show that with prefix-free parsing we can build an 131-MB run-length compressed FM-index (restricted to support only counting and not locating) for 1000 copies of human chromosome 19 in 2 h using 21  GB of memory, suggesting that we can build a 6.73 GB index for 1000 complete human-genome haplotypes in approximately 102 h using about 1 TB of memory.","Burrows-Wheeler Transform, Prefix-free parsing, Compression-aware algorithms, Genomic databases","CISE, University of Florida, Gainesville, FL, USAChristina Boucher & Alan KuhnleEIT, Diego Portales University, Santiago, ChileTravis GagieCeBiB, Santiago, ChileTravis GagieInformatics Institute, Gainesville, FL, USAAlan KuhnleJohns Hopkins University, Baltimore, MD, USABen Langmead & Taher MunUniversity of Eastern Piedmont, Alessandria, ItalyGiovanni ManziniIIT, CNR, Pisa, ItalyGiovanni Manzini",Christina Boucher,NA
Linear time minimum segmentation enables scalable founder reconstruction,17-May-19,"Tuukka Norri, Bastien Cazaux, Dmitry Kosolobov and Veli Mäkinen",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0147-6,"Background We study a preprocessing routine relevant in pan-genomic analyses: consider a set of aligned haplotype sequences of complete human chromosomes. Due to the enormous size of such data, one would like to represent this input set with a few founder sequences that retain as well as possible the contiguities of the original sequences. Such a smaller set gives a scalable way to exploit pan-genomic information in further analyses (e.g. read alignment and variant calling). Optimizing the founder set is an NP-hard problem, but there is a segmentation formulation that can be solved in polynomial time, defined as follows. Given a threshold L and a set \({\mathcal {R}} = \{R_1, \ldots , R_m\}\) of m strings (haplotype sequences), each having length n, the minimum segmentation problem for founder reconstruction is to partition [1, n] into set P of disjoint segments such that each segment \([a,b] \in P\) has length at least L and the number \(d(a,b)=|\{R_i[a,b] :1\le i \le m\}|\) of distinct substrings at segment [a, b] is minimized over \([a,b] \in P\). The distinct substrings in the segments represent founder blocks that can be concatenated to form \(\max \{ d(a,b) :[a,b] \in P \}\) founder sequences representing the original \({\mathcal {R}}\) such that crossovers happen only at segment boundaries.Results We give an O(mn) time (i.e. linear time in the input size) algorithm to solve the minimum segmentation problem for founder reconstruction, improving over an earlier \(O(mn^2)\).Conclusions Our improvement enables to apply the formulation on an input of thousands of complete human chromosomes. We implemented the new algorithm and give experimental evidence on its practicality. The implementation is available in https://github.com/tsnorri/founder-sequences.","Pan-genome indexing, Founder reconstruction, Dynamic programming, Positional Burrows–Wheeler transform, Range minimum query","Department of Computer Science, University of Helsinki, Pietari Kalmin katu 5, 00014, Helsinki, FinlandTuukka Norri, Bastien Cazaux & Veli MäkinenUral Federal University, Mira 19, Yekaterinburg, 620002, RussiaDmitry Kosolobov",Veli Mäkinen,NA
An average-case sublinear forward algorithm for the haploid Li and Stephens model,02-Apr-19,Yohei M. Rosen and Benedict J. Paten,https://almob.biomedcentral.com/articles/10.1186/s13015-019-0144-9,"BackgroundHidden Markov models of haplotype inheritance such as the Li and Stephens model allow for computationally tractable probability calculations using the forward algorithm as long as the representative reference panel used in the model is sufficiently small. Specifically, the monoploid Li and Stephens model and its variants are linear in reference panel size unless heuristic approximations are used. However, sequencing projects numbering in the thousands to hundreds of thousands of individuals are underway, and others numbering in the millions are anticipated.ResultsTo make the forward algorithm for the haploid Li and Stephens model computationally tractable for these datasets, we have created a numerically exact version of the algorithm with observed average case sublinear runtime with respect to reference panel size k when tested against the 1000 Genomes dataset.ConclusionsWe show a forward algorithm which avoids any tradeoff between runtime and model complexity. Our algorithm makes use of two general strategies which might be applicable to improving the time complexity of other future sequence analysis algorithms: sparse dynamic programming matrices and lazy evaluation.","Forward algorithm, Haplotype, Complexity, Sublinear algorithms","UCSC Genomics Institute, 1156 High St, Santa Cruz, CA, 95064, USAYohei M. RosenNYU School of Medicine, 550 First Ave, New York, NY, 10016, USAYohei M. Rosen & Benedict J. Paten",Yohei M. Rosen,NA
Differentially mutated subnetworks discovery,30-Mar-19,"Morteza Chalabi Hajkarim, Eli Upfal and Fabio Vandin",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0146-7,"ProblemWe study the problem of identifying differentially mutated subnetworks of a large gene–gene interaction network, that is, subnetworks that display a significant difference in mutation frequency in two sets of cancer samples. We formally define the associated computational problem and show that the problem is NP-hard.AlgorithmWe propose a novel and efficient algorithm, called DAMOKLE, to identify differentially mutated subnetworks given genome-wide mutation data for two sets of cancer samples. We prove that DAMOKLE identifies subnetworks with statistically significant difference in mutation frequency when the data comes from a reasonable generative model, provided enough samples are available.Experimental resultsWe test DAMOKLE on simulated and real data, showing that DAMOKLE does indeed find subnetworks with significant differences in mutation frequency and that it provides novel insights into the molecular mechanisms of the disease not revealed by standard methods.","Network analysis, Somatic mutations, Differential analysis","Biotech Research and Innovation Centre, University of Copenhagen, Copenhagen, DenmarkMorteza Chalabi HajkarimDepartment of Computer Science, Brown University, Providence, RI, USAEli UpfalDepartment of Information Engineering, University of Padova, Padova, ItalyFabio Vandin",Fabio Vandin,NA
Repairing Boolean logical models from time-series data using Answer Set Programming,25-Mar-19,"Alexandre Lemos, Inês Lynce and Pedro T. Monteiro",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0145-8,"BackgroundBoolean models of biological signalling-regulatory networks are increasingly used to formally describe and understand complex biological processes. These models may become inconsistent as new data become available and need to be repaired. In the past, the focus has been shed on the inference of (classes of) models given an interaction network and time-series data sets. However, repair of existing models against new data is still in its infancy, where the process is still manually performed and therefore slow and prone to errors.ResultsIn this work, we propose a method with an associated tool to suggest repairs over inconsistent Boolean models, based on a set of atomic repair operations. Answer Set Programming is used to encode the minimal repair problem as a combinatorial optimization problem. In particular, given an inconsistent model, the tool provides the minimal repairs that render the model capable of generating dynamics coherent with a (set of) time-series data set(s), considering either a synchronous or an asynchronous updating scheme.ConclusionsThe method was validated using known biological models from different species, as well as synthetic models obtained from randomly generated networks. We discuss the method’s limitations regarding each of the updating schemes and the considered minimization algorithm.","Biological regulatory networks, Boolean functions, Model repair, (A)synchronous dynamics, Answer Set Programming","INESC-ID/Instituto Superior Técnico, Universidade de Lisboa, Rua Alves Redol 9, 1000-029, Lisbon, PortugalAlexandre Lemos, Inês Lynce & Pedro T. Monteiro",Alexandre Lemos,NA
Kermit: linkage map guided long read assembly,20-Mar-19,"Riku Walve, Pasi Rastas and Leena Salmela",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0143-x,"Background With long reads getting even longer and cheaper, large scale sequencing projects can be accomplished without short reads at an affordable cost. Due to the high error rates and less mature tools, de novo assembly of long reads is still challenging and often results in a large collection of contigs. Dense linkage maps are collections of markers whose location on the genome is approximately known. Therefore they provide long range information that has the potential to greatly aid in de novo assembly. Previously linkage maps have been used to detect misassemblies and to manually order contigs. However, no fully automated tools exist to incorporate linkage maps in assembly but instead large amounts of manual labour is needed to order the contigs into chromosomes.Results We formulate the genome assembly problem in the presence of linkage maps and present the first method for guided genome assembly using linkage maps. Our method is based on an additional cleaning step added to the assembly. We show that it can simplify the underlying assembly graph, resulting in more contiguous assemblies and reducing the amount of misassemblies when compared to de novo assembly.Conclusions We present the first method to integrate linkage maps directly into genome assembly. With a modest increase in runtime, our method improves contiguity and correctness of genome assembly.","Genome assembly, Linkage maps, Coloured overlap graph","Department of Computer Science, Helsinki Institute for Information Technology HIIT, University of Helsinki, Helsinki, FinlandRiku Walve & Leena SalmelaInstitute of Biotechnology, University of Helsinki, Helsinki, FinlandPasi Rastas",Riku Walve,NA
Reconciling multiple genes trees via segmental duplications and losses,20-Mar-19,"Riccardo Dondi, Manuel Lafond and Celine Scornavacca",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0139-6,"Reconciling gene trees with a species tree is a fundamental problem to understand the evolution of gene families. Many existing approaches reconcile each gene tree independently. However, it is well-known that the evolution of gene families is interconnected. In this paper, we extend a previous approach to reconcile a set of gene trees with a species tree based on segmental macro-evolutionary events, where segmental duplication events and losses are associated with cost \(\delta \) and \(\lambda \), respectively. We show that the problem is polynomial-time solvable when \(\delta \le \lambda \) (via LCA-mapping), while if \(\delta > \lambda \) the problem is NP-hard, even when \(\lambda = 0\) and a single gene tree is given, solving a long standing open problem on the complexity of multi-gene reconciliation. On the positive side, we give a fixed-parameter algorithm for the problem, where the parameters are \(\delta /\lambda \) and the number d of segmental duplications, of time complexity \(O\left(\lceil \frac{\delta }{\lambda } \rceil ^{d} \cdot n \cdot \frac{\delta }{\lambda }\right)\). Finally, we demonstrate the usefulness of this algorithm on two previously studied real datasets: we first show that our method can be used to confirm or raise doubt on hypothetical segmental duplications on a set of 16 eukaryotes, then show how we can detect whole genome duplications in yeast genomes.","Phylogenetics, Gene trees, Species trees, Reconciliation, Segmental duplications, Fixed-parameter tractability, NP-hardness, Whole genome duplications","Dipartimento di Filosofia, Lettere, Comunicazione, Università degli Studi di Bergamo, Bergamo, ItalyRiccardo DondiDepartment of Computer Science, Universitè de Sherbrooke, Sherbrooke, CanadaManuel LafondISEM, CNRS, IRD, EPHE, Universit de Montpellier, Montpellier, FranceCeline Scornavacca",Manuel Lafond,NA
External memory BWT and LCP computation for sequence collections with applications,08-Mar-19,"Lavinia Egidi, Felipe A. Louza, Giovanni Manzini and Guilherme P. Telles",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0140-0,"BackgroundSequencing technologies produce larger and larger collections of biosequences that have to be stored in compressed indices supporting fast search operations. Many compressed indices are based on the Burrows–Wheeler Transform (BWT) and the longest common prefix (LCP) array. Because of the sheer size of the input it is important to build these data structures in external memory and time using in the best possible way the available RAM.ResultsWe propose a space-efficient algorithm to compute the BWT and LCP array for a collection of sequences in the external or semi-external memory setting. Our algorithm splits the input collection into subcollections sufficiently small that it can compute their BWT in RAM using an optimal linear time algorithm. Next, it merges the partial BWTs in external or semi-external memory and in the process it also computes the LCP values. Our algorithm can be modified to output two additional arrays that, combined with the BWT and LCP array, provide simple, scan-based, external memory algorithms for three well known problems in bioinformatics: the computation of maximal repeats, the all pairs suffix–prefix overlaps, and the construction of succinct de Bruijn graphs.ConclusionsWe prove that our algorithm performs \({\mathcal {O}}(n\, \mathsf {maxlcp})\) sequential I/Os, where n is the total length of the collection and \(\mathsf {maxlcp}\) is the maximum LCP value. The experimental results show that our algorithm is only slightly slower than the state of the art for short sequences but it is up to 40 times faster for longer sequences or when the available RAM is at least equal to the size of the input.","Burrows–Wheeler Transform, Longest common prefix array, Maximal repeats, All pairs suffix–prefix overlaps, Succinct de Bruijn graph, External memory algorithms","DiSIT, University of Eastern Piedmont, Viale Michel, 11, 15121, Alessandria, ItalyLavinia Egidi & Giovanni ManziniDepartment of Computing and Mathematics, University of São Paulo, Av. Bandeirantes, 3900, 14040-901, Ribeirão Preto, BrazilFelipe A. LouzaIIT CNR, Via Moruzzi, 1, 56124, Pisa, ItalyGiovanni ManziniInstitute of Computing, University of Campinas, Av. Albert Einstein, 1251, 13083-852, Campinas, BrazilGuilherme P. Telles",Felipe A. Louza,NA
Connectivity problems on heterogeneous graphs,08-Mar-19,"Jimmy Wu, Alex Khodaverdian, Benjamin Weitz and Nir Yosef",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0141-z,"BackgroundNetwork connectivity problems are abundant in computational biology research, where graphs are used to represent a range of phenomena: from physical interactions between molecules to more abstract relationships such as gene co-expression. One common challenge in studying biological networks is the need to extract meaningful, small subgraphs out of large databases of potential interactions. A useful abstraction for this task turned out to be the Steiner Network problems: given a reference “database” graph, find a parsimonious subgraph that satisfies a given set of connectivity demands. While this formulation proved useful in a number of instances, the next challenge is to account for the fact that the reference graph may not be static. This can happen for instance, when studying protein measurements in single cells or at different time points, whereby different subsets of conditions can have different protein milieu.Results and discussionWe introduce the condition Steiner Network problem in which we concomitantly consider a set of distinct biological conditions. Each condition is associated with a set of connectivity demands, as well as a set of edges that are assumed to be present in that condition. The goal of this problem is to find a minimal subgraph that satisfies all the demands through paths that are present in the respective condition. We show that introducing multiple conditions as an additional factor makes this problem much harder to approximate. Specifically, we prove that for C conditions, this new problem is NP-hard to approximate to a factor of \(C - \epsilon \), for every \(C \ge 2\) and \(\epsilon > 0\), and that this bound is tight. Moving beyond the worst case, we explore a special set of instances where the reference graph grows monotonically between conditions, and show that this problem admits substantially improved approximation algorithms. We also developed an integer linear programming solver for the general problem and demonstrate its ability to reach optimality with instances from the human protein interaction network.ConclusionOur results demonstrate that in contrast to most connectivity problems studied in computational biology, accounting for multiplicity of biological conditions adds considerable complexity, which we propose to address with a new solver. Importantly, our results extend to several network connectivity problems that are commonly used in computational biology, such as Prize-Collecting Steiner Tree, and provide insight into the theoretical guarantees for their applications in a multiple condition setting.","Steiner Network, NP hard, Approximation algorithm, Protein–protein interaction","Department of Computer Science, Stanford University, Stanford, CA, USAJimmy WuDepartment of Electrical Engineering and Computer Science, UC Berkeley, Berkeley, CA, USAAlex Khodaverdian, Benjamin Weitz & Nir Yosef",Nir Yosef,NA
Semi-nonparametric modeling of topological domain formation from epigenetic data,05-Mar-19,Emre Sefer and Carl Kingsford,https://almob.biomedcentral.com/articles/10.1186/s13015-019-0142-y,"BackgroundHi-C experiments capturing the 3D genome architecture have led to the discovery of topologically-associated domains (TADs) that form an important part of the 3D genome organization and appear to play a role in gene regulation and other functions. Several histone modifications have been independently associated with TAD formation, but their combinatorial effects on domain formation remain poorly understood at a global scale.ResultsWe propose a convex semi-nonparametric approach called nTDP based on Bernstein polynomials to explore the joint effects of histone markers on TAD formation as well as predict TADs solely from the histone data. We find a small subset of modifications to be predictive of TADs across species. By inferring TADs using our trained model, we are able to predict TADs across different species and cell types, without the use of Hi-C data, suggesting their effect is conserved. This work provides the first comprehensive joint model of the effect of histone markers on domain formation.ConclusionsOur approach, nTDP, can form the basis of a unified, explanatory model of the relationship between epigenetic marks and topological domain structures. It can be used to predict domain boundaries for cell types, species, and conditions for which no Hi-C data is available. The model may also be of use for improving Hi-C-based domain finders.","Chromatin, Conformation, Capture, Topological domains, Epigenetic modifications, Prediction","Machine Learning Department, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 15213, USAEmre SeferComputational Biology Department, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 15213, USACarl Kingsford",Emre Sefer,NA
SNPs detection by eBWT positional clustering,06-Feb-19,"Nicola Prezza, Nadia Pisanti, Marinella Sciortino and Giovanna Rosone",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0137-8,"BackgroundSequencing technologies keep on turning cheaper and faster, thus putting a growing pressure for data structures designed to efficiently store raw data, and possibly perform analysis therein. In this view, there is a growing interest in alignment-free and reference-free variants calling methods that only make use of (suitably indexed) raw reads data.ResultsWe develop the positional clustering theory that (i) describes how the extended Burrows–Wheeler Transform (eBWT) of a collection of reads tends to cluster together bases that cover the same genome position (ii) predicts the size of such clusters, and (iii) exhibits an elegant and precise LCP array based procedure to locate such clusters in the eBWT. Based on this theory, we designed and implemented an alignment-free and reference-free SNPs calling method, and we devised a consequent SNPs calling pipeline. Experiments on both synthetic and real data show that SNPs can be detected with a simple scan of the eBWT and LCP arrays as, in accordance with our theoretical framework, they are within clusters in the eBWT of the reads. Finally, our tool intrinsically performs a reference-free evaluation of its accuracy by returning the coverage of each SNP.ConclusionsBased on the results of the experiments on synthetic and real data, we conclude that the positional clustering framework can be effectively used for the problem of identifying SNPs, and it appears to be a promising approach for calling other type of variants directly on raw sequencing data.AvailabilityThe software ebwt2snp is freely available for academic use at: https://github.com/nicolaprezza/ebwt2snp.","BWT, LCP array, SNPs, Reference-free, Assembly-free","Dipartimento di Informatica, University of Pisa, Pisa, ItalyNicola Prezza, Nadia Pisanti & Giovanna RosoneDipartimento di Matematica e Informatica, University of Palermo, Palermo, ItalyMarinella SciortinoERABLE Team, INRIA, Lyon, FranceNadia Pisanti",Giovanna Rosone,NA
Constrained incremental tree building: new absolute fast converging phylogeny estimation methods with improved scalability and accuracy,06-Feb-19,"Qiuyi Zhang, Satish Rao and Tandy Warnow",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0136-9,"BackgroundAbsolute fast converging (AFC) phylogeny estimation methods are ones that have been proven to recover the true tree with high probability given sequences whose lengths are polynomial in the number of number of leaves in the tree (once the shortest and longest branch weights are fixed). While there has been a large literature on AFC methods, the best in terms of empirical performance was \(DCM_{NJ},\) published in SODA 2001. The main empirical advantage of \({DCM}_{NJ}\) over other AFC methods is its use of neighbor joining (NJ) to construct trees on smaller taxon subsets, which are then combined into a tree on the full set of species using a supertree method; in contrast, the other AFC methods in essence depend on quartet trees that are computed independently of each other, which reduces accuracy compared to neighbor joining. However, \({DCM}_{NJ}\) is unlikely to scale to large datasets due to its reliance on supertree methods, as no current supertree methods are able to scale to large datasets with high accuracy.ResultsIn this study we present a new approach to large-scale phylogeny estimation that shares some of the features of \({DCM}_{NJ}\) but bypasses the use of supertree methods. We prove that this new approach is AFC and uses polynomial time and space. Furthermore, we describe variations on this basic approach that can be used with leaf-disjoint constraint trees (computed using methods such as maximum likelihood) to produce other methods that are likely to provide even better accuracy. Thus, we present a new generalizable technique for large-scale tree estimation that is designed to improve scalability for phylogeny estimation methods to ultra-large datasets, and that can be used in a variety of settings (including tree estimation from unaligned sequences, and species tree estimation from gene trees).","Phylogeny estimation, Short quartets, Sample complexity, Absolute fast converging methods, Neighbor joining, Maximum likelihood","Department of Mathematics, University of California Berkeley, Evans Hall, Berkeley, CA, 94720, USAQiuyi ZhangDepartment of Computer Science, University of California Berkeley, SODA Hall, Berkeley, CA, 94720, USASatish RaoDepartment of Computer Science, University of Illinois Urbana-Champaign, 201 N. Goodwin Avenue, Urbana, IL, 61801, USATandy Warnow",Tandy Warnow,NA
Automated partial atomic charge assignment for drug-like molecules: a fast knapsack approach,05-Feb-19,"Martin S. Engler, Bertrand Caron, Lourens Veen, Daan P. Geerke, Alan E. Mark and Gunnar W. Klau",https://almob.biomedcentral.com/articles/10.1186/s13015-019-0138-7,"A key factor in computational drug design is the consistency and reliability with which intermolecular interactions between a wide variety of molecules can be described. Here we present a procedure to efficiently, reliably and automatically assign partial atomic charges to atoms based on known distributions. We formally introduce the molecular charge assignment problem, where the task is to select a charge from a set of candidate charges for every atom of a given query molecule. Charges are accompanied by a score that depends on their observed frequency in similar neighbourhoods (chemical environments) in a database of previously parameterised molecules. The aim is to assign the charges such that the total charge equals a known target charge within a margin of error while maximizing the sum of the charge scores. We show that the problem is a variant of the well-studied multiple-choice knapsack problem and thus weakly \(\mathcal {NP}\)-complete. We propose solutions based on Integer Linear Programming and a pseudo-polynomial time Dynamic Programming algorithm. We demonstrate that the results obtained for novel molecules not included in the database are comparable to the ones obtained performing explicit charge calculations while decreasing the time to determine partial charges for a molecule from hours or even days to below a second. Our software is openly available.","Multiple-choice knapsack, Integer Linear Programming, Pseudo-polynomial Dynamic Programming, Partial charge assignment, Molecular dynamics simulations","Algorithmic Bioinformatics, Heinrich Heine University, Universitätsstr. 1, 40225, Düsseldorf, GermanyMartin S. Engler & Gunnar W. KlauLife Sciences and Health Group, Centrum Wiskunde & Informatica, Science Park 123, 1098 XG, Amsterdam, The NetherlandsMartin S. EnglerSchool of Chemistry & Molecular Biosciences, The University of Queensland, Brisbane, QLD,  4072, AustraliaBertrand Caron & Alan E. MarkNetherlands eScience Center, Science Park 140, 1098 XG, Amsterdam, The NetherlandsLourens VeenAIMMS Division of Molecular and Computational Toxicology, Vrije Universiteit, De Boelelaan 1108, 1081 HZ, Amsterdam, The NetherlandsDaan P. Geerke",Gunnar W. Klau,NA
